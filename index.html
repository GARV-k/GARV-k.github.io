<!DOCTYPE html>
<html>

<head>
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta name="google-site-verification" content="ONL58rZnCLtdQDM-rXI9vLreOYlM_esgpUYTRUQsUzM" />
	<title>Garv Kaushik</title>
	<link rel="icon" href="images\assassin-s-creed-symbol-logo-free-vector.jpg">

	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@200&display=swap" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@400&display=swap" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@500&display=swap" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&family=Space+Mono:ital@1&display=swap"
		rel="stylesheet">
	<link href="style.css" rel="stylesheet">
</head>

<body>
	<div class="container">
		<img class="profile-img" src="images\portfolio_bannerimg.jpg" alt="Profile Picture">
		<div>
			<h1>Garv Kaushik <span style="font-size: 14px; color: #666; font-weight: normal; display: inline-block; margin-left: 10px;">[ garvkaushik1 at gmail dot com ]</span></h1>
			<p class="contact"> <a class="custom-link" href="mailto:garvkaushik1@gmail.com">Contact</a> | <a
					class="custom-link" href="https://www.github.com/GARV-k"> GitHub</a> | <a
					class="custom-link" href="https://scholar.google.com/citations?user=_-SnJssAAAAJ&hl=en"> Scholar</a> | 
					<a class="custom-link" href="https://www.linkedin.com/in/garv-kaushik-52278624b/"> LinkedIn</a> |
					<a class="custom-link" href="https://drive.google.com/file/d/1yKCyEsmQGVmRb8bkVBrFfcwBSVTXSB1B/view"> CV</a>
				</p>

			<div class="bio">
				<p>I am a 3rd year undergrad at IIT (BHU) Varanasi studying Math and Computing.
					 <!-- <a class="custom-link"
						href="https://team.inria.fr/titane/pierre-alliez/">Pierre Alliez</a> (<a class="custom-link"
						href="https://www.inria.fr/fr/titane">Titane team</a>) and <a class="custom-link"
						href="https://pages.saclay.inria.fr/mathieu.desbrun/">Mathieu Desbrun</a> (<a
						class="custom-link" href="https://www.inria.fr/fr/geomerix">Geomerix team</a>).  -->
						I love to work on ideas involving Deep Learning and have a particular itch for Geometric Deep Learning
						and 3D Computer Vision. Very recently I have been working with (then a PhD student)
						<a class="custom-link" href="https://scholar.google.co.in/citations?user=HE2nfp0AAAAJ&hl=en">Pinaki Nath Chowdhury</a>
						from <a class="custom-link" href="https://scholar.google.com/citations?user=irZFP_AAAAAJ&hl=en">Yi-Zhe Song's group</a> - 
						<a class="custom-link" href="https://sketchx.eecs.qmul.ac.uk/">SketchX</a> to develop an idea of Implicit Animations 
						for Signed Distance Fields which could be used to enable rig-free animations in the long run.
				</p>
			</div>
		</div>
	</div>

	<h2>News & Updates</h2>
	<div class="timeline">
		<div class="timeline-item">
			<span class="timeline-date">April 2024</span>
			<span class="timeline-badge">New!</span>
			<div class="timeline-content">
				Awarded the prestigious Charpak Summer Training Scholarship'25 for Funded Research in France ! ðŸŽ‰
			</div>
		</div>
			<div class="timeline-item">
			<span class="timeline-date">Feb 2025</span>
			<span class="timeline-badge">New!</span>
			<div class="timeline-content">
				Paper on "ViBe: Text-to-Video Benchmark" accepted at NAACL Workshop - TrustNLP 2025! ðŸŽ‰
			</div>
		</div>
		<div class="timeline-item">
			<span class="timeline-date">July 2024</span>
			<div class="timeline-content">
				Starting research internship at SketchX Lab, University of Surrey
			</div>
		</div>
	</div>

	<h2>Research Papers</h2>

	<!-- <ul class="pub-line"></ul> -->
	<!-- <ul class="pub-line"></ul> -->
	<ul class="pub">
		<div class="container">
			<div class="zoomhover">
				<a href="https://vibe-t2v-bench.github.io/">
					<img class="pub-img" src="images\vidhal.gif" alt="Publication 1 Image" width="200">
				</a>
			</div>
			<div>
				<a class="pub-title" href="https://vibe-t2v-bench.github.io/"> ViBe: A Text-to-Video Benchmark for Evaluating Hallucination in Large Multimodal Models
				</a>
				<div class="pub-authors"> Vipula Rawte,  Sarthak Jain, Aarush Sinha, <b>Garv Kaushik</b>, et al. </div>

				<div class="pub-abstract">
					We introduce ViBe: a large-scale Text-to-Video Benchmark of hallucinated videos from T2V models. We identify five major types of hallucination: Vanishing Subject, Numeric Variability, Temporal Dysmorphia, Omission Error, and Physical Incongruity. Using 10 open-source T2V models, we developed the first large-scale dataset of hallucinated videos, comprising 3,782 videos annotated by humans into these five categories.

ViBe offers a unique resource for evaluating the reliability of T2V models and provides a foundation for improving hallucination detection and mitigation in video generation. We establish classification as a baseline and present various ensemble classifier configurations, with the TimeSFormer + CNN combination yielding the best performance, achieving 0.345 accuracy and 0.342 F1 score. This benchmark aims to drive the development of robust T2V models that produce videos more accurately aligned with input prompts.
				</div>
				<div class="pub-conf"> Proc. NAACL Workshop - TrustNLP 2025</div>
			</div>
		</div>
	</ul>

	<ul class="pub-line"></ul>

	<ul class="pub">
		<div class="container">
			<div class="zoomhover">
				<a href="">
					<img class="pub-img" src="images\Screenshot 2025-04-16 161548.png" alt="Publication 1 Image" width="200">
				</a>
			</div>
			<div>
				<a class="pub-title" href=""> Exploring Aapative Structure Learning for Heterophilic Graphs</a>
				<div class="pub-authors"> <b>Garv Kaushik</b>, Darsh Kaushik</div>

				<div class="pub-abstract">
					Graph Convolutional Networks (GCNs) gained traction for graph representation learning, with recent attention on 
					improving performance on heterophilic graphs for various real-world applications. The localized feature aggregation
					 in a typical message-passing paradigm hinders the capturing of long-range dependencies between non-local nodes 
					 of the same class. The inherent connectivity structure in heterophilic graphs often conflicts with information 
					 sharing between distant nodes of same class. We propose structure learning to rewire edges in shallow GCNs itself
					  to avoid performance degradation in downstream discriminative tasks due to oversmoothing. Parameterizing 
					  the adjacency matrix to learn connections between non-local nodes and extend the hop span of shallow GCNs 
					  facilitates the capturing of long-range dependencies. However, our method is not generalizable across 
					  heterophilic graphs and performs inconsistently on node classification task contingent to the graph structure.
				</div>
				<div class="pub-conf"> Manuscript in preparation</div>
			</div>
		</div>
	</ul>

	<ul class="pub-line"></ul>

	<!-- <ul class="pub">

		<div class="container">
			<div class="zoomhover">
				<a href="https://nissmar.github.io/voromesh.github.io/">
					<img class="pub-img" src="img/Voromesh.png" alt="Publication 1 Image">
				</a>
			</div>
			<div>
				<a class="pub-title" href="https://nissmar.github.io/voromesh.github.io/"> VoroMesh: Learning Watertight
					Surface Meshes with Voronoi Diagrams</a>
				<div class="pub-authors"> <b>Nissim Maruani</b>, Roman Klokov, Maks Ovsjanikov, Pierre Alliez, Mathieu
					Desbrun </div>

				<div class="pub-abstract">
					In stark contrast to the case of images, finding a concise, learnable discrete representation of 3D
					surfaces remains a challenge. In particular, while polygon meshes are arguably the most common
					surface representation used in geometry processing, their irregular and combinatorial structure
					often make them unsuitable for learning-based applications. In this work, we present VoroMesh, a
					novel and differentiable Voronoi-based representation of water- tight 3D shape surfaces. From a set
					of 3D points (called generators) and their associated occupancy, we define our boundary
					representation through the Voronoi diagram of the generators as the subset of Voronoi faces whose
					two associated (equidistant) generators are of opposite occupancy: the resulting polygon mesh forms
					a watertight approximation of the target shapeâ€™s boundary. To learn the position of the generators,
					we propose a novel loss function, dubbed VoroLoss, that minimizes the distance from groundtruth
					surface samples to the closest faces of the Voronoi diagram which does not require an explicit
					construction of the entire Voronoi diagram. A direct optimization of the Voroloss to obtain
					generators on the Thingi32 dataset demonstrates the geometric efficiency of our representation
					compared to axiomatic meshing algorithms and recent learning-based mesh representations. We further
					use VoroMesh in a learning-based mesh prediction task from input SDF grids on the ABC dataset, and
					show comparable performance to state-of-the-art methods while guaranteeing closed output surfaces
					free of self-intersections.
				</div>
				<div class="pub-conf"> Proc. International Conference on Computer Vision (ICCV), 2023</div>

			</div>
		</div>

	</ul>

	<ul class="pub-line"></ul>

	<ul class="pub">

		<div class="container">
			<div class="zoomhover">
				<a href="https://inria.hal.science/hal-04135266#">
					<img class="pub-img" src="img/offset.png" alt="Publication 1 Image">
				</a>
			</div>
			<div>
				<a class="pub-title" href="https://inria.hal.science/hal-04135266#"> Feature-Preserving Offset Mesh
					Generation from Topology-Adapted Octrees</a>
				<div class="pub-authors">Daniel Zint, <b>Nissim Maruani</b>, Mael Rouxel-LabbÃ©, Pierre Alliez</div>

				<div class="pub-abstract">We introduce a reliable method to generate offset meshes from input triangle
					meshes or triangle soups. Our method proceeds in two steps. The first step performs a Dual
					Contouring method on the offset surface, operating on an adaptive octree that is refined in areas
					where the offset topology is complex. Our approach substantially reduces memory consumption and
					runtime compared to isosurfacing methods operating on uniform grids. The second step improves the
					output Dual Contouring mesh with an offset-aware remeshing algorithm to reduce the normal deviation
					between the mesh facets and the exact offset. This remeshing process reconstructs concave sharp
					features and approximates smooth shapes in convex areas up to a user-defined precision. We show the
					effectiveness and versatility of our method by applying it to a wide range of input meshes. We also
					benchmark our method on the entire Thingi10k dataset: watertight, 2-manifold offset meshes are
					obtained for 100% of the cases.</div>
				<div class="pub-conf">Symposium on geometry processing (SGP), 2023</div>

			</div>

		</div>
	</ul> -->
	<!-- <ul class="pub-line"></ul> -->

	<!-- <h2>Teaching</h2>
	<ul>
		<li> ENS-PSL (2022-Today, Paris, France): exploring new ways of teaching maths with <a class="custom-link"
				href="https://mathadata.fr/">MathAData</a> </li>
		<li>
			<a class="custom-link" href="https://www.larotonde-sciences.com">La Rotonde</a> (2018-2019, Saint-Ã‰tienne,
			France): Science Mediation at <a class="custom-link" href="https://fondation-lamap.org/en">La main Ã  la
				pÃ¢te</a>
		</li>
	</ul> -->
	<h2>Experience</h2>
	<ul>
		<li>
			<span class="duration">[July 2024 - Present]</span> Research Intern @ <a class="custom-link" href="https://sketchx.eecs.qmul.ac.uk/">SketchX Lab, University of Surrey</a>
			<br>
			<span class="supervisor">Supervisor: <a class="custom-link" href="https://scholar.google.co.in/citations?user=HE2nfp0AAAAJ&hl=en">Dr. Pinaki Nath Chowdhury</a></span>
		</li>
		<li>
			<span class="duration">[May 2024 - July 2024]</span> Summer Research Intern @ <a class="custom-link" href="https://mlrgisi.github.io/">ML-Lab, Indian Statistical Institute</a>
			<br>
			<span class="supervisor">Supervisor: <a class="custom-link" href="https://www.isical.ac.in/~swagatam.das/main.htm">Dr. Swagatam Das </a></span>
		</li>
		<li>
			<span class="duration">[Decemeber 2023- March  2024]</span> AI-Research Intern @ <a class="custom-link" href="https://www.mirrar.com/">mirrAR</a>
			<br>
			<span class="supervisor">Supervisor: <a class="custom-link" href="https://scholar.google.co.in/citations?user=f7yB-GMAAAAJ&hl=en">Dr. Mustafa Sameer </a></span>
		</li>
	</ul>
	<h2>Communities</h2>
	<ul>
		<li> Core Team Member (2023-24) @ <a class="custom-link" href="https://cops-iitbhu.github.io/IG-website/">Club of Programmers (IIT BHU) - Intelligence Group</a></li>
		<li>
			Core Team Member (2023-24) @ <a class="custom-link" href="https://www.copsiitbhu.co.in/">Club of Programmers (IIT BHU) - Infosec Group</a>
		</li>
	</ul>
	

	<!-- <h2>Reviewer</h2>
	<ul>
		<li>BMVC 2024, SIGGRAPH ASIA 2024, SIGGRAPH 2024, BMVC 2023</li>
	</ul> -->

	<!-- <h2>Education</h2>
	<ul>
		<li> ENS Paris-Saclay (2021-2022, Gif-sur-Yvette, France): Master MVA</li>
		<li> Ã‰cole polytechnique (2018-2022, Saclay, France): Engineering Curriculum </li>
		<li> LycÃ©e Louis-le-Grand (2016-2018, Paris, France): "Classe prÃ©pa" </li>
	</ul> -->


	<h2>Other-projects</h2>

	<div class="container project">
		<a href="https://github.com/GARV-k/FHE-watermark-net" class="code-project zoomhover">
			<img class="code-img" src="images\Untitled.png" height="200" width = "400">
			<p class="code-title custom-link">AI-FHE Invisible Image Watermarking</p>
		</a>
		<a href="https://github.com/manas1245agrawal/video_question_answering/tree/main" class="code-project zoomhover">
			<img class="code-img" src="images\0_nB_MGMu4oVCSzk34.png" height = "200" width = "500">
			<p class="code-title custom-link">VQA for GIFs</p>
		</a>
		<a href="https://github.com/aaravm/neural_ode?tab=readme-ov-file" class="code-project zoomhover">
			<img class="code-img" src="images\Neural_ODE _project.svg" height="250">
			<p class="code-title custom-link">Neural-ODE</p>
		</a>
		<a href="https://drive.google.com/drive/folders/1-FZrqbwGDSBtuGGu8MEPqi8HIalgrrPv" class="code-project zoomhover">
			<img class="code-img" src="images\VQA_project.png">
			<p class="code-title custom-link">VQA for Biomed-images</p>
		</a>
		<a href="https://github.com/GARV-k/Image-Denoiser/tree/main" class="code-project zoomhover">
			<img class="code-img" src="images\Image_denoiserproject.png">
			<p class="code-title custom-link">Image Denoising-Autoencoders</p>
		</a>

	</div>

	<h2>Readings</h2>
	<ul>
			<li>
				<span class="duration">[NLP & Multimodal Learning]</span>
				<ul>
					<li>Attention is All You Need (<a class="custom-link" href="https://arxiv.org/abs/1706.03762">Paper</a> )</li>
					<li>Semantically Grounded QFormer for Efficient Vision Language Understanding (<a class="custom-link" href="https://arxiv.org/abs/2311.07449">Paper</a> )	</li>
				</ul>
			</li>
			<li>
				<span class="duration">[2D/3D Computer Vision]</span>
				<ul>
					<li>NeRF: Neural Radiance Fields (<a class="custom-link" href="#">Paper</a> )</li>
					<li>MeshSDF(<a class="custom-link" href="https://arxiv.org/abs/2006.03997">Paper</a> )</li>
					<li>DeepSDF(<a class="custom-link" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Park_DeepSDF_Learning_Continuous_Signed_Distance_Functions_for_Shape_Representation_CVPR_2019_paper.html">Paper</a> )</li>
					<li>Instant-NGP (<a class="custom-link" href="https://nvlabs.github.io/instant-ngp/assets/mueller2022instant.pdf">Paper</a> )</li>
					<li>PCEDNet : A Lightweight Neural Network for Fast and Interactive Edge Detection in 3D Point Clouds (<a class="custom-link" href="https://arxiv.org/abs/2011.01630">Paper</a> )</li>
					<li>IM Avatar: Implicit Morphable Head Avatars from Videos (<a class="custom-link" href="https://arxiv.org/abs/2112.07471">Paper</a> )</li>	
					<li>InstructScene : Instruction-Driven 3D Indoor Scene Synthesis with Semantic Graph Prior (<a class="custom-link" href="https://arxiv.org/abs/2402.04717">Paper</a> )</li>
					<li>(VQ-VAE) Neural Discrete Representation Learning (<a class="custom-link" href="https://arxiv.org/abs/1711.00937">Paper</a> )</li>
					<li>TEMOS: Generating diverse human motions from textual descriptions(<a class="custom-link" href="https://arxiv.org/abs/2204.14109">Paper</a> )</li>		
				</ul>
			</li>
			<li>
				<span class="duration">GNNs & Graph Transformers</span>
				<ul>
					<li>Semi-Supervised Classification with Graph Convolutional Networks
						(<a class="custom-link" href="https://arxiv.org/abs/1609.02907">Paper</a> )</li>
					<li>A Generalization of Transformer Networks to Graphs (<a class="custom-link" href="https://arxiv.org/pdf/2012.09699">Paper</a> )</li>
					<li>NodeFormer: A Scalable Graph Structure Learning Transformer for Node Classification  (<a class="custom-link" href="https://openreview.net/forum?id=sMezXGG5So">Paper</a> )</li>
					
				</ul>
			</li>
	</ul>



</body>

</html>